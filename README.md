# ensemble_methods
Bagging and Boosting -Random Forest and AdaBoost
![ensemble](https://user-images.githubusercontent.com/89722385/143453031-5c9b01ab-13dc-4c6e-942e-2859bb1d01d3.jpeg)


Decision trees are trained for one specific task or dataset and cannot be transferred to
another similar problem. Nevertheless, several decision trees can be combined in order
to create bigger models and learn how to generalize. These are called random forests.
The name forest refers to an ensemble of many decision tree algorithms, following the
<b><font color="blue">bagging</font></b> method, which states that the combination of several algorithms achieves the
best result overall. The appearance of the word "random" refers to the randomness of
the algorithm when selecting the features to take into account to split a node.
